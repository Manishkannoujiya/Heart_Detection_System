# -*- coding: utf-8 -*-
"""DiseaseDetectation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nmipLSX7Md7hVzd90144PO2uJLKHHgo4
"""

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

heart_df = pd.read_csv('heart.csv')
heart_df

heart_df.sample(5)

heart_df.info()

heart_df.describe()

heart_df.describe(include="all")

heart_df.isna().sum()

heart_df.isnull()

heart_df.isnull().sum()

heart_df.duplicated().sum()

heart_df.nunique()

heart_df['ChestPainType'].unique()

range(heart_df['ChestPainType'].nunique())

heart_df.columns

cat_col = heart_df.select_dtypes(include="object").columns

for col in cat_col:
    print(col)
    unique_values = heart_df[col].unique()
    # Create a dictionary mapping unique values to integers
    mapping = {value: i for i, value in enumerate(unique_values)}
    print(unique_values, list(mapping.values()))
    heart_df[col].replace(mapping, inplace=True)
    print('*'*90)
    print()

heart_df['Cholesterol'].value_counts()

np.nan

heart_df['Cholesterol'].replace(0,np.nan,inplace=True)

from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=3)
after_impute = imputer.fit_transform(heart_df)
heart_df = pd.DataFrame(after_impute,columns=heart_df.columns)

heart_df

heart_df['Cholesterol'].isna().sum()

count = 0
for i in heart_df['Cholesterol']:
  if i == 0:
    count += 1
print(count)

heart_df['RestingBP'][heart_df['RestingBP'] == 0]

from sklearn.impute import KNNImputer
heart_df['RestingBP'].replace(0, np.nan, inplace = True)
imputer = KNNImputer(n_neighbors=3)
after_impute = imputer.fit_transform(heart_df)
heart_df = pd.DataFrame(after_impute,columns=heart_df.columns)

heart_df['RestingBP'].unique()

heart_df['RestingBP'].isnull().sum()

withoutOldPeak = heart_df.columns
withoutOldPeak = withoutOldPeak.drop('Oldpeak')
heart_df[withoutOldPeak] = heart_df[withoutOldPeak].astype('int32')

heart_df.info()

heart_df.sample()

heart_df.corr()

"""**Data Visualization**"""

!pip install plotly

import plotly.express as px

px.line(heart_df.corr()['HeartDisease'][: -1].sort_values())

"""Age and HeartDisease Distribution"""

px.sunburst(heart_df,path=['HeartDisease','Age'])

px.histogram(heart_df, x='Age',color='HeartDisease')

"""Percentage of HeartDisease data distribution"""

px.pie(heart_df, names='HeartDisease', title = 'Percentage of HeartDisease classes distribution')

"""Sex vs Heart Disease

"""

px.histogram(heart_df, x='Sex',color='HeartDisease')

"""ChestPain Type vs HeartDisease"""

px.histogram(heart_df, x='ChestPainType',color='HeartDisease')

"""RestingBP vs HeartDisease

"""

heart_df['RestingBP'].unique()

px.sunburst(heart_df,path = ['HeartDisease','RestingBP'])

"""FastingBS vs Heart Disease

"""

px.histogram(heart_df, x='FastingBS',color='HeartDisease')

"""MaxHR vs Heart Disease

"""

px.sunburst(heart_df,path = ['HeartDisease','MaxHR'])

px.violin(heart_df,x='HeartDisease',y='MaxHR', color='HeartDisease')

"""OldPeak vs Heart Disease

"""

px.violin(heart_df,x='HeartDisease',y='Oldpeak', color='HeartDisease')

"""ST_SLope vs Heart Disease"""

px.histogram(heart_df, x='ST_Slope',color='HeartDisease')

"""ExerciseAngina vs Heart Disease"""

px.histogram(heart_df, x='ExerciseAngina',color='HeartDisease')

"""Train Test Split"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(heart_df.drop('HeartDisease', axis=1), heart_df['HeartDisease'], test_size=0.2, random_state=42,
stratify=heart_df['HeartDisease'])

"""Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
solver = ['lbfgs' , 'liblinear' , 'newton-cg' , 'newton-cholesky' , 'sag' , 'saga']
best_solver = ''
test_score = np.zeros(6)
for i, n in enumerate(solver) :
  lr = LogisticRegression(solver = n).fit(X_train, y_train)
  test_score[i] = lr.score(X_test, y_test)
  if lr.score(X_test, y_test) == max(test_score) :
    best_solver = n

    print(best_solver)
    lr = LogisticRegression(solver=best_solver)
    lr.fit(X_train, y_train)
    lr_pred = lr.predict(X_test)
    print(f'LogisticRegression Score: {accuracy_score(y_test, lr_pred)}')

import joblib
file = open('LogisticR.joblib','wb')
joblib.dump(lr,file)

"""Upport Vector Machine(SVM)"""

from sklearn.svm import SVC
from sklearn.metrics import f1_score
kernels = {'linear' :0,'poly' : 0, 'rbf':0 , 'sigmoid':0}
best = ''
for i in kernels:
  svm = SVC(kernel=i)
  svm.fit(X_train,y_train)
  yhat = svm.predict(X_test)
  kernels[i] = f1_score(y_test,yhat , average="weighted")
  if kernels[i] == max(kernels.values()):
    best = i
svm = SVC(kernel=best)
svm.fit(X_train,y_train)
svm_pred = svm.predict(X_test)
print(f' SVM f1_score kernal({best}) : {f1_score(y_test, svm_pred, average="weighted")}')

import joblib
file = open('SVM.joblib','wb')
joblib.dump(lr,file)

"""Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

dtree = DecisionTreeClassifier(class_weight = 'balanced')
param_grid = {
    'max_depth' : [3,4,5,6,7,8],
    'min_samples_split' : [2,3,4],
    'min_samples_leaf' : [1,2,3,4],
    'random_state' : [0,42]
}
grid_search = GridSearchCV(dtree, param_grid, cv=5)
grid_search.fit(X_train,y_train)
Ctree = DecisionTreeClassifier(**grid_search.best_params_, class_weight='balanced')
Ctree.fit(X_train,y_train)
dtc_pred = Ctree.predict(X_test)
print("DecisionTrees's Accuracy :" , accuracy_score(y_test,dtc_pred ) )

import joblib
file = open('DecisionTreeC.joblib','wb')
joblib.dump(lr,file)

"""Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

rfc = RandomForestClassifier()
param_grid = {
    'n_estimators' : [50,100,150,500],
    'max_features' : ['sqrt' , 'log2' , None],
    'max_depth' : [3,6,9,19],
    'max_leaf_nodes' : [3,6,9],
}
grid_search = GridSearchCV(rfc , param_grid)
grid_search.fit(X_train,y_train)
rfctree = RandomForestClassifier(**grid_search.best_params_)
rfctree.fit(X_train,y_train)
rfc_pred = Ctree.predict(X_test)
print("RandomForestClassifier's Accuracy :" , accuracy_score(y_test,rfc_pred ) )

import pickle
file = open('RandomForest.pkl','wb')
pickle.dump(lr,file)

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
import pickle
X = heart_df.drop('HeartDisease', axis=1)
y = heart_df['HeartDisease']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
rf = RandomForestClassifier(random_state=42)
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}
grid_rf = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_rf.fit(X_train, y_train)
grid_rf = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_rf.fit(X_train, y_train)
print("GridSearchCV model saved as gridrf.pkl")
print("Best Parameters:", grid_rf.best_params_)

import joblib
file = open('joblib.pkl','rb')
joblib.dump(lr,file)